# Performance Optimization Guide

## üöÄ Otimiza√ß√µes Implementadas

### 1. ‚úÖ RAG Cache
**Localiza√ß√£o:** `src/rag/retriever.py`

- Cache em mem√≥ria para resultados de busca RAG
- Chave: `(query[:160], discipline, k)`
- FIFO eviction quando atinge 128 entradas
- **Impacto:** Reduz 30 chamadas RAG ‚Üí 3 chamadas (uma por quest√£o)

**Logs:**
```
RAG: cache hit
```

---

### 2. ‚úÖ Paraleliza√ß√£o Otimizada
**Localiza√ß√£o:** `src/utils/helpers.py`

- Semaphore aumentado de **2 ‚Üí 10** (default)
- Configur√°vel via `API_CONCURRENCY` env var
- Throttle configur√°vel via `API_THROTTLE_SLEEP` (default 0.2s)

**Como ajustar:**
```bash
export API_CONCURRENCY=15  # Aumenta paralelismo
export API_THROTTLE_SLEEP=0.1  # Reduz delay entre chamadas
```

**Aten√ß√£o:**
- Se receber **429 (Rate Limit)**: reduza `API_CONCURRENCY` ou aumente `API_THROTTLE_SLEEP`
- Gemini free-tier: recomendado `API_CONCURRENCY=5` e `API_THROTTLE_SLEEP=0.5`

---

### 3. ‚úÖ Performance Logging
**Localiza√ß√£o:** 
- `src/utils/helpers.py` (`measure_time` context manager)
- `src/agents/dspy_examiner.py`
- `src/agents/dspy_arbiter.py`
- `src/rag/retriever.py`

**Logs de Tempo:**
```
‚è≥ Iniciando: DSPy Examiner CORRETOR_1 - Question Q1...
‚úÖ Conclu√≠do: DSPy Examiner CORRETOR_1 - Question Q1 em 3.2456 segundos.

‚è≥ Iniciando: RAG Retrieval (Similarity Search)...
‚úÖ Conclu√≠do: RAG Retrieval (Similarity Search) em 0.1234 segundos.
```

**Como usar:**
```python
from src.utils.helpers import measure_time

with measure_time("Nome da Opera√ß√£o"):
    # c√≥digo aqui
    pass
```

---

### 4. ‚úÖ Streamlit Progress Streaming
**Localiza√ß√£o:** `app/main.py`

- Progress bars durante gera√ß√£o de quest√µes
- Progress bars durante simula√ß√£o de respostas
- Progress bars durante corre√ß√£o
- Status containers com feedback visual em tempo real

**Features:**
- `st.status()` com expans√£o autom√°tica
- `st.progress()` com percentual
- Mensagens de feedback por fase (RAG, Corretor 1, Corretor 2, √Årbitro)

---

### 5. ‚úÖ Batch Processing Otimizado
**Localiza√ß√£o:** `app/main.py` (Batch Processing mode)

**Estrat√©gias:**
- **Chunking:** Processa alunos em lotes (default: 4)
- **Cooldown:** Pausa entre lotes para evitar rate limits
- **Paralelismo por quest√£o:** Todos os alunos respondem Q1 em paralelo, depois Q2, etc.

**Configura√ß√£o:**
```bash
export BATCH_CHUNK_SIZE=4  # Alunos por lote
export BATCH_COOLDOWN_SLEEP=0.0  # Pausa entre lotes (segundos)
```

**Exemplo de fluxo:**
1. **Gerar 5 quest√µes** ‚Üí 5 chamadas LLM sequenciais
2. **Simular 30 alunos √ó 5 quest√µes:**
   - Loop: Q1 ‚Üí 30 alunos em paralelo (usando `safe_gather`)
   - Loop: Q2 ‚Üí 30 alunos em paralelo
   - ...
   - Total: 150 chamadas em 5 lotes
3. **Corrigir 30 alunos √ó 5 quest√µes:**
   - Q1: 30 corre√ß√µes em chunks de 4 ‚Üí ~8 lotes
   - Cada corre√ß√£o = 2 corretores + √°rbitro (se divergir)
   - Total: ~300-450 chamadas (dependendo de diverg√™ncias)

---

## üìä Benchmarks Esperados

### Antes da Otimiza√ß√£o
- 10 alunos √ó 3 quest√µes = **~10 minutos**
- Rate limits frequentes (429)
- UI congelada sem feedback

### Depois da Otimiza√ß√£o
- 10 alunos √ó 3 quest√µes = **~2-3 minutos**
- Cache RAG reduz 90% das buscas
- Paralelismo 5x maior (2 ‚Üí 10)
- Feedback visual em tempo real

### Gemini Free-Tier (Recomendado)
```bash
export API_CONCURRENCY=5
export API_THROTTLE_SLEEP=0.5
export BATCH_CHUNK_SIZE=3
export BATCH_COOLDOWN_SLEEP=1.0
```

Tempo esperado: **~4-5 minutos** para 10 alunos √ó 3 quest√µes

---

## üîç Como Monitorar Performance

### 1. Logs de Tempo
```bash
streamlit run app/main.py 2>&1 | grep "Conclu√≠do:"
```

### 2. Cache Hits
```bash
streamlit run app/main.py 2>&1 | grep "cache hit"
```

### 3. Rate Limits
```bash
streamlit run app/main.py 2>&1 | grep -E "(429|rate limit)"
```

---

## üõ†Ô∏è Troubleshooting

### Erro: 429 Too Many Requests
**Solu√ß√£o:**
```bash
export API_CONCURRENCY=3
export API_THROTTLE_SLEEP=1.0
export BATCH_COOLDOWN_SLEEP=2.0
```

### Lentid√£o: Cache n√£o est√° funcionando
**Debug:**
```python
# Adicione em retriever.py
logger.info(f"Cache size: {len(_RAG_CACHE)}")
logger.info(f"Cache keys: {list(_RAG_CACHE.keys())}")
```

### UI congelando no Streamlit
**Causa:** Loop de evento bloqueado  
**Solu√ß√£o:** Todas as opera√ß√µes async j√° usam `run_async()` que cria novo event loop

---

## üìà Pr√≥ximas Otimiza√ß√µes (Opcional)

### 1. Persistent Cache (Redis/SQLite)
- Cache sobrevive a restarts do Streamlit
- Compartilh√°vel entre sess√µes

### 2. Embedding Cache
- Cachear embeddings de quest√µes
- Reduz lat√™ncia de RAG ainda mais

### 3. Batch Embeddings
- Processar m√∫ltiplos textos em uma chamada
- Suportado por OpenAI e Gemini

### 4. LLM Response Caching
- Cachear respostas do LLM por (question, answer, rubric)
- √ötil para demonstra√ß√µes repetidas

### 5. Observability Dashboard
- Grafana/Prometheus para m√©tricas em tempo real
- Tokens consumidos, lat√™ncias, cache hit rate

---

## üìù Changelog

### 2026-02-10
- ‚úÖ Aumentado `API_CONCURRENCY` de 2 ‚Üí 10
- ‚úÖ Adicionado `measure_time` em DSPy Examiner e Arbiter
- ‚úÖ Cache RAG j√° estava implementado (verificado)
- ‚úÖ Streamlit progress j√° estava implementado (verificado)
- ‚úÖ Documenta√ß√£o criada (este arquivo)

---

**Autor:** Alan Turing (AI Assistant)  
**Projeto:** ai-grading-system (TCC Lucas Savino)  
**Repo:** https://github.com/savinoo/ai-grading-system
